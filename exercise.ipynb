{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing codebase embeddings with a projection matrix\n",
    "\n",
    "## Goal\n",
    "\n",
    "The outcome of this exercise is to learn a projection matrix that tailors embeddings for a codebase retrieval use case, and then measure the improvement in retrieval quality.\n",
    "\n",
    "The notebook is mostly filled out, but has a series of small gaps that you will need to fill in (everywhere you see a \"TODO\" comment):\n",
    "- Define the similarity functions (both basic and with projection matrix)\n",
    "- Define a suitable loss function\n",
    "- Construct examples for training from the pre-existing dataset\n",
    "- Complete the training loop code\n",
    "- Finish the retrieval function logic\n",
    "- Evaluate the improvement in retrieval quality\n",
    "\n",
    "## Background\n",
    "\n",
    "A basic retrieval augmented generation (RAG) system will typically use embeddings to represent a set of documents that are to be searched over. Then the user input can also be converted to an embedding, and the system will use the dot product of the two embeddings to determine the relevance of the input to the documents in the database.\n",
    "\n",
    "Many embedding models are \"symmetric\", which means that they treat user input text and documents (e.g. code snippets) in the same way. It might be preferable to calculate the embedding differently (\"asymmetrically\") for the user input because is is a fundamentally different type of text.\n",
    "\n",
    "One way of doing this is to use the same embedding model, and then apply a matrix multiplication to the embedding of the user input. What we'll try to do here is find such a matrix that can improve retrieval quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "We recommend using a virtual environment to install the necessary packages.\n",
    "\n",
    "```bash\n",
    "python -m venv env\n",
    "source env/bin/activate\n",
    "```\n",
    "\n",
    "### Install packages\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Here we generate a sample embedding with `sentence_transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 384\n",
      "Embedding: [-0.07597316801548004, -0.005261966027319431, ..., 0.03495463728904724]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained model (this will be slow the first time)\n",
    "model = SentenceTransformer(\"all-MiniLM-L12-v2\")\n",
    "\n",
    "def embed(text):\n",
    "    embedding = model.encode([text])[0]\n",
    "    return torch.tensor(embedding, dtype=torch.float32)\n",
    "\n",
    "embedding = embed(\"Hello world\")\n",
    "dim = len(embedding)\n",
    "\n",
    "print(f\"Embedding dimension: {dim}\")\n",
    "print(f\"Embedding: [{embedding[0]}, {embedding[1]}, ..., {embedding[-1]}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity\n",
    "\n",
    "First, we'll define our definition of similarity. This can be calculated using a dot product between two embeddings. For example, if we were trying to find the similarity between a user input $x_i$ and a code snippet $x_c$, then the similarity would be\n",
    "\n",
    "$$h(x_i, x_c) = e(x_i) \\cdot e(x_c)$$\n",
    "\n",
    "Fill out the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity 1: 0.4509845972061157\n",
      "Similarity 2: 0.03275971859693527\n"
     ]
    }
   ],
   "source": [
    "# Define the similarity function using torch\n",
    "def similarity(x_i, x_c):\n",
    "    embedded_x_i = model.encode(x_i)\n",
    "    embedded_x_i = torch.tensor(embedded_x_i, dtype=torch.float32)\n",
    "    embedded_x_c = model.encode(x_c)\n",
    "    embedded_x_c = torch.tensor(embedded_x_c, dtype=torch.float32)\n",
    "\n",
    "    similarity = torch.dot(embedded_x_i, embedded_x_c)\n",
    "    return similarity\n",
    "\n",
    "# Calculate the similarity between two strings\n",
    "x_i = \"Where in the codebase do we do auth?\"\n",
    "x_c_1 = \"```python\\n# Authentication\\ndef authenticate(username, password):\\n    # Code to authenticate the user\\n```\"\n",
    "x_c_2 = \"function sum(a, b) {\\n    return a + b;\\n}\"\n",
    "\n",
    "similarity1 = similarity(x_i, x_c_1)\n",
    "similarity2 = similarity(x_i, x_c_2)\n",
    "print(f\"Similarity 1: {similarity1}\")\n",
    "print(f\"Similarity 2: {similarity2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity with projection matrix\n",
    "\n",
    "Next, we'll calculate similarity using the projection matrix\n",
    "\n",
    "$$h_\\theta(x_i, x_c) = e(x_c) \\theta e(x_i)$$\n",
    "\n",
    "Fill in the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2487, -0.7133,  0.0479,  ..., -1.1818,  1.3128, -1.1162],\n",
      "        [-0.3665, -0.4251, -1.0567,  ...,  1.0513,  1.9071, -0.6326],\n",
      "        [-0.8089,  0.8485, -0.0504,  ..., -2.1244, -1.2786, -0.9352],\n",
      "        ...,\n",
      "        [-1.1284, -1.4597,  2.3380,  ...,  0.5578,  0.8412,  0.3509],\n",
      "        [-0.0792,  0.3729, -0.3419,  ...,  1.3249,  0.5113, -0.5444],\n",
      "        [-1.5188, -1.1874,  0.2557,  ..., -1.1737,  1.4754,  1.2180]])\n",
      "Similarity with projection 1: -0.28043413162231445\n",
      "Similarity with projection 2: -0.3947674036026001\n"
     ]
    }
   ],
   "source": [
    "def similarity_with_projection(x_i, x_c, P):\n",
    "    embedded_x_i = model.encode(x_i)\n",
    "    embedded_x_c = model.encode(x_c)\n",
    "\n",
    "    embedded_x_i = torch.tensor(embedded_x_i, dtype=torch.float32)\n",
    "    embedded_x_c = torch.tensor(embedded_x_c, dtype=torch.float32).squeeze()\n",
    "\n",
    "    projected = torch.matmul(P, embedded_x_i)\n",
    "    similarity = torch.dot(projected, embedded_x_c)\n",
    "    return similarity\n",
    "\n",
    "# Generate a dim by dim random matrix\n",
    "P_random = torch.randn(dim, dim, dtype=torch.float32)\n",
    "print(P_random)\n",
    "\n",
    "# Calculate the similarity with the random projection matrix\n",
    "similarity_with_projection1 = similarity_with_projection(x_i, x_c_1, P_random)\n",
    "similarity_with_projection2 = similarity_with_projection(x_i, x_c_2, P_random)\n",
    "print(f\"Similarity with projection 1: {similarity_with_projection1}\")\n",
    "print(f\"Similarity with projection 2: {similarity_with_projection2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "To train and test a matrix that is more helpful than the random one above, we will use a pre-existing dataset, which includes a list of (question, relevant code snippets) pairs, which happen to have been generated by a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from XML file (dataset.xml)\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass \n",
    "class Example:\n",
    "    user_input: str\n",
    "    snippets: List[str]\n",
    "\n",
    "class DatasetParser:\n",
    "    def __init__(self, xml_file: str):\n",
    "        self.tree = ET.parse(xml_file)\n",
    "        self.root = self.tree.getroot()\n",
    "\n",
    "    def parse(self) -> List[Example]:\n",
    "        examples = []\n",
    "        \n",
    "        for example in self.root.findall('example'):\n",
    "            user_input = example.find('user_input').text\n",
    "            snippets_list = []\n",
    "            \n",
    "            for snippet in example.find('snippets').findall('snippet'):\n",
    "                # Extract code and filename from the snippet text\n",
    "                snippet_text = snippet.text.strip()\n",
    "                \n",
    "                # Parse the filename from the code block header\n",
    "                first_line = snippet_text.split('\\n')[0]\n",
    "                filename = first_line.split(' ')[1] if len(first_line.split(' ')) > 1 else None\n",
    "                \n",
    "                # Remove the code block markers and get just the code\n",
    "                code_lines = snippet_text.split('\\n')[1:-1]\n",
    "                code = '\\n'.join(code_lines)\n",
    "                \n",
    "                snippets_list.append(code)\n",
    "                \n",
    "            examples.append(Example(\n",
    "                user_input=user_input,\n",
    "                snippets=snippets_list\n",
    "            ))\n",
    "            \n",
    "        return examples\n",
    "\n",
    "\n",
    "parser = DatasetParser('dataset.xml')\n",
    "dataset = parser.parse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Example(user_input='How do we handle password reset flows?', snippets=['def initiate_password_reset(email):\\n    token = generate_reset_token()\\n    send_reset_email(email, token)\\n    store_reset_token(email, token, expiry=24*hours)\\n    return True\\n\\ndef validate_reset_token(token, new_password):\\n    if is_token_valid(token):\\n        user = get_user_by_token(token)\\n        update_password(user, new_password)\\n        invalidate_token(token)\\n        return True\\n    return False']), Example(user_input='Where is the user registration logic implemented?', snippets=['export class UserService {\\n  async register(userData: RegisterDTO): Promise<User> {\\n    const existingUser = await this.userRepo.findByEmail(userData.email);\\n    if (existingUser) {\\n      throw new DuplicateUserError();\\n    }\\n    \\n    const hashedPassword = await bcrypt.hash(userData.password);\\n    return this.userRepo.create({\\n      ...userData,\\n      password: hashedPassword\\n    });\\n  }\\n}']), Example(user_input='How do we process payments?', snippets=[\"class PaymentProcessor {\\n  async processPayment(amount: number, paymentMethod: PaymentMethod): Promise<PaymentResult> {\\n    const transaction = await this.stripeClient.createCharge({\\n      amount,\\n      currency: 'usd',\\n      payment_method: paymentMethod.id,\\n      confirm: true\\n    });\\n    \\n    return this.saveTransaction(transaction);\\n  }\\n}\"]), Example(user_input='Where is the email sending functionality?', snippets=[\"from sendgrid import SendGridAPIClient\\n\\nclass EmailService:\\n    def __init__(self, api_key):\\n        self.client = SendGridAPIClient(api_key)\\n    \\n    def send_email(self, to_email, subject, content):\\n        message = {\\n            'to': to_email,\\n            'subject': subject,\\n            'content': content,\\n            'from_email': 'noreply@ourapp.com'\\n        }\\n        return self.client.send(message)\"]), Example(user_input='How do we handle file uploads?', snippets=[\"@Controller('uploads')\\nexport class FileUploadController {\\n  @Post()\\n  async uploadFile(@UploadedFile() file: Express.Multer.File) {\\n    const result = await this.s3Service.upload(file.buffer, {\\n      bucket: 'user-uploads',\\n      contentType: file.mimetype\\n    });\\n    \\n    return { url: result.Location };\\n  }\\n}\"]), Example(user_input='Where is the database connection configuration?', snippets=[\"module.exports = {\\n  development: {\\n    client: 'postgresql',\\n    connection: {\\n      host: process.env.DB_HOST,\\n      database: process.env.DB_NAME,\\n      user: process.env.DB_USER,\\n      password: process.env.DB_PASSWORD\\n    },\\n    pool: {\\n      min: 2,\\n      max: 10\\n    }\\n  }\\n}\"]), Example(user_input='How do we implement rate limiting?', snippets=[\"export class RateLimiter {\\n  constructor(private redis: Redis) {}\\n\\n  async limit(req: Request, res: Response, next: NextFunction) {\\n    const key = `rate_limit:${req.ip}`;\\n    const requests = await this.redis.incr(key);\\n    \\n    if (requests === 1) {\\n      await this.redis.expire(key, 60);\\n    }\\n    \\n    if (requests > 100) {\\n      return res.status(429).json({ error: 'Too many requests' });\\n    }\\n    \\n    next();\\n  }\\n}\"]), Example(user_input='Where is the search functionality implemented?', snippets=[\"export class SearchService {\\n  async search(query: string, filters: SearchFilters): Promise<SearchResult[]> {\\n    const elasticQuery = this.buildElasticQuery(query, filters);\\n    \\n    const results = await this.elasticClient.search({\\n      index: 'products',\\n      body: elasticQuery\\n    });\\n    \\n    return this.mapResults(results.hits);\\n  }\\n}\"]), Example(user_input='How do we handle logging?', snippets=[\"import winston from 'winston';\\n\\nexport const logger = winston.createLogger({\\n  level: process.env.LOG_LEVEL || 'info',\\n  format: winston.format.json(),\\n  transports: [\\n    new winston.transports.File({ filename: 'error.log', level: 'error' }),\\n    new winston.transports.File({ filename: 'combined.log' })\\n  ]\\n});\"]), Example(user_input='Where is the caching logic?', snippets=[\"export class CacheService {\\n  constructor(private redis: Redis) {}\\n  \\n  async get<T>(key: string): Promise<T | null> {\\n    const cached = await this.redis.get(key);\\n    return cached ? JSON.parse(cached) : null;\\n  }\\n  \\n  async set(key: string, value: any, ttl?: number): Promise<void> {\\n    await this.redis.set(key, JSON.stringify(value), 'EX', ttl || 3600);\\n  }\\n}\"]), Example(user_input='How do we handle websocket connections?', snippets=[\"@WebSocketGateway()\\nexport class WebSocketGateway {\\n  @WebSocketServer()\\n  server: Server;\\n\\n  handleConnection(client: Socket) {\\n    const userId = this.getUserFromToken(client.handshake.auth.token);\\n    client.join(`user:${userId}`);\\n  }\\n\\n  @SubscribeMessage('message')\\n  handleMessage(client: Socket, payload: any) {\\n    this.server.to(payload.room).emit('message', payload);\\n  }\\n}\"]), Example(user_input='Where is the input validation handled?', snippets=['export class ValidationMiddleware {\\n  validate(schema: Joi.Schema) {\\n    return (req: Request, res: Response, next: NextFunction) => {\\n      const { error } = schema.validate(req.body);\\n      \\n      if (error) {\\n        return res.status(400).json({\\n          error: error.details[0].message\\n        });\\n      }\\n      \\n      next();\\n    };\\n  }\\n}']), Example(user_input='How do we implement the shopping cart?', snippets=['export class CartService {\\n  async addToCart(userId: string, productId: string, quantity: number) {\\n    const cart = await this.cartRepo.findByUser(userId);\\n    const product = await this.productRepo.findById(productId);\\n    \\n    if (product.stock < quantity) {\\n      throw new InsufficientStockError();\\n    }\\n    \\n    return this.cartRepo.addItem(cart.id, {\\n      productId,\\n      quantity,\\n      price: product.price\\n    });\\n  }\\n}']), Example(user_input='Where is the notification system implemented?', snippets=['export class NotificationService {\\n  async notify(userId: string, notification: Notification) {\\n    const user = await this.userRepo.findById(userId);\\n    \\n    if (user.preferences.email) {\\n      await this.emailService.send(user.email, notification);\\n    }\\n    \\n    if (user.preferences.push) {\\n      await this.pushService.send(user.deviceToken, notification);\\n    }\\n  }\\n}']), Example(user_input='How do we handle API errors?', snippets=[\"export class ErrorHandler {\\n  catch(error: Error, req: Request, res: Response, next: NextFunction) {\\n    if (error instanceof ValidationError) {\\n      return res.status(400).json({\\n        type: 'ValidationError',\\n        message: error.message\\n      });\\n    }\\n    \\n    if (error instanceof AuthenticationError) {\\n      return res.status(401).json({\\n        type: 'AuthenticationError',\\n        message: 'Unauthorized'\\n      });\\n    }\\n    \\n    // Default error\\n    res.status(500).json({\\n      type: 'ServerError',\\n      message: 'Internal server error'\\n    });\\n  }\\n}\"]), Example(user_input='Where is the user profile update logic?', snippets=['export class ProfileService {\\n  async updateProfile(userId: string, updates: ProfileUpdates): Promise<User> {\\n    const user = await this.userRepo.findById(userId);\\n    \\n    if (updates.email && updates.email !== user.email) {\\n      await this.verifyEmailUnique(updates.email);\\n    }\\n    \\n    const updatedUser = await this.userRepo.update(userId, updates);\\n    return this.sanitizeUser(updatedUser);\\n  }\\n}']), Example(user_input='How do we generate PDFs?', snippets=[\"export class PDFGenerator {\\n  async generatePDF(template: string, data: any): Promise<Buffer> {\\n    const html = await this.templateEngine.render(template, data);\\n    \\n    const browser = await puppeteer.launch();\\n    const page = await browser.newPage();\\n    await page.setContent(html);\\n    \\n    const pdf = await page.pdf({\\n      format: 'A4',\\n      printBackground: true\\n    });\\n    \\n    await browser.close();\\n    return pdf;\\n  }\\n}\"]), Example(user_input='Where is the background job processing handled?', snippets=[\"export class JobProcessor {\\n  @Process('email')\\n  async processEmailJob(job: Job) {\\n    const { to, subject, content } = job.data;\\n    await this.emailService.send(to, subject, content);\\n  }\\n\\n  @Process('image-resize')\\n  async processImageResize(job: Job) {\\n    const { imageUrl, dimensions } = job.data;\\n    const resized = await this.imageService.resize(imageUrl, dimensions);\\n    await this.storageService.upload(resized);\\n  }\\n}\"]), Example(user_input='How do we implement the recommendation system?', snippets=['class RecommendationEngine:\\n    def generate_recommendations(self, user_id):\\n        user_history = self.get_user_history(user_id)\\n        similar_users = self.find_similar_users(user_history)\\n        \\n        recommendations = []\\n        for user in similar_users:\\n            items = self.get_user_items(user)\\n            recommendations.extend(self.filter_unseen_items(items, user_history))\\n        \\n        return self.rank_recommendations(recommendations)']), Example(user_input='Where is the analytics tracking implemented?', snippets=['export class AnalyticsService {\\n  async trackEvent(event: AnalyticsEvent) {\\n    await this.segmentClient.track({\\n      userId: event.userId,\\n      event: event.name,\\n      properties: event.properties,\\n      timestamp: new Date()\\n    });\\n    \\n    await this.store.incrementEventCount(event.name);\\n  }\\n}'])]\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct examples\n",
    "\n",
    "Convert the dataset into a set of examples that can be used to train the projection matrix. These should include both examples of input/snippet pairs where the snippet is relevant, and pairs where the snippet is not relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, you should generate a list of positive and negative pairs from the dataset\n",
    "# These will be used to train the matrix\n",
    "import random\n",
    "\n",
    "# TODO: Create example pairs from the dataset\n",
    "example_pairs = []  # list of tuples (user input, code snippet, 1 if snippet is relevant to user input else 0)\n",
    "\n",
    "# creates equal amount of 1/0 pairs\n",
    "for example in dataset:\n",
    "    example_pairs.append((example.user_input, example.snippets, 1))\n",
    "    # irrelevant\n",
    "    others = [ex for ex in dataset if ex != example]\n",
    "    rexample = random.choice(others)\n",
    "    example_pairs.append((example.user_input, random.choice(rexample.snippets), 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('How do we handle password reset flows?', ['def initiate_password_reset(email):\\n    token = generate_reset_token()\\n    send_reset_email(email, token)\\n    store_reset_token(email, token, expiry=24*hours)\\n    return True\\n\\ndef validate_reset_token(token, new_password):\\n    if is_token_valid(token):\\n        user = get_user_by_token(token)\\n        update_password(user, new_password)\\n        invalidate_token(token)\\n        return True\\n    return False'], 1), ('How do we handle password reset flows?', \"class PaymentProcessor {\\n  async processPayment(amount: number, paymentMethod: PaymentMethod): Promise<PaymentResult> {\\n    const transaction = await this.stripeClient.createCharge({\\n      amount,\\n      currency: 'usd',\\n      payment_method: paymentMethod.id,\\n      confirm: true\\n    });\\n    \\n    return this.saveTransaction(transaction);\\n  }\\n}\", 0), ('Where is the user registration logic implemented?', ['export class UserService {\\n  async register(userData: RegisterDTO): Promise<User> {\\n    const existingUser = await this.userRepo.findByEmail(userData.email);\\n    if (existingUser) {\\n      throw new DuplicateUserError();\\n    }\\n    \\n    const hashedPassword = await bcrypt.hash(userData.password);\\n    return this.userRepo.create({\\n      ...userData,\\n      password: hashedPassword\\n    });\\n  }\\n}'], 1), ('Where is the user registration logic implemented?', 'export class AnalyticsService {\\n  async trackEvent(event: AnalyticsEvent) {\\n    await this.segmentClient.track({\\n      userId: event.userId,\\n      event: event.name,\\n      properties: event.properties,\\n      timestamp: new Date()\\n    });\\n    \\n    await this.store.incrementEventCount(event.name);\\n  }\\n}', 0), ('How do we process payments?', [\"class PaymentProcessor {\\n  async processPayment(amount: number, paymentMethod: PaymentMethod): Promise<PaymentResult> {\\n    const transaction = await this.stripeClient.createCharge({\\n      amount,\\n      currency: 'usd',\\n      payment_method: paymentMethod.id,\\n      confirm: true\\n    });\\n    \\n    return this.saveTransaction(transaction);\\n  }\\n}\"], 1), ('How do we process payments?', 'export class UserService {\\n  async register(userData: RegisterDTO): Promise<User> {\\n    const existingUser = await this.userRepo.findByEmail(userData.email);\\n    if (existingUser) {\\n      throw new DuplicateUserError();\\n    }\\n    \\n    const hashedPassword = await bcrypt.hash(userData.password);\\n    return this.userRepo.create({\\n      ...userData,\\n      password: hashedPassword\\n    });\\n  }\\n}', 0), ('Where is the email sending functionality?', [\"from sendgrid import SendGridAPIClient\\n\\nclass EmailService:\\n    def __init__(self, api_key):\\n        self.client = SendGridAPIClient(api_key)\\n    \\n    def send_email(self, to_email, subject, content):\\n        message = {\\n            'to': to_email,\\n            'subject': subject,\\n            'content': content,\\n            'from_email': 'noreply@ourapp.com'\\n        }\\n        return self.client.send(message)\"], 1), ('Where is the email sending functionality?', \"import winston from 'winston';\\n\\nexport const logger = winston.createLogger({\\n  level: process.env.LOG_LEVEL || 'info',\\n  format: winston.format.json(),\\n  transports: [\\n    new winston.transports.File({ filename: 'error.log', level: 'error' }),\\n    new winston.transports.File({ filename: 'combined.log' })\\n  ]\\n});\", 0), ('How do we handle file uploads?', [\"@Controller('uploads')\\nexport class FileUploadController {\\n  @Post()\\n  async uploadFile(@UploadedFile() file: Express.Multer.File) {\\n    const result = await this.s3Service.upload(file.buffer, {\\n      bucket: 'user-uploads',\\n      contentType: file.mimetype\\n    });\\n    \\n    return { url: result.Location };\\n  }\\n}\"], 1), ('How do we handle file uploads?', 'export class NotificationService {\\n  async notify(userId: string, notification: Notification) {\\n    const user = await this.userRepo.findById(userId);\\n    \\n    if (user.preferences.email) {\\n      await this.emailService.send(user.email, notification);\\n    }\\n    \\n    if (user.preferences.push) {\\n      await this.pushService.send(user.deviceToken, notification);\\n    }\\n  }\\n}', 0), ('Where is the database connection configuration?', [\"module.exports = {\\n  development: {\\n    client: 'postgresql',\\n    connection: {\\n      host: process.env.DB_HOST,\\n      database: process.env.DB_NAME,\\n      user: process.env.DB_USER,\\n      password: process.env.DB_PASSWORD\\n    },\\n    pool: {\\n      min: 2,\\n      max: 10\\n    }\\n  }\\n}\"], 1), ('Where is the database connection configuration?', 'class RecommendationEngine:\\n    def generate_recommendations(self, user_id):\\n        user_history = self.get_user_history(user_id)\\n        similar_users = self.find_similar_users(user_history)\\n        \\n        recommendations = []\\n        for user in similar_users:\\n            items = self.get_user_items(user)\\n            recommendations.extend(self.filter_unseen_items(items, user_history))\\n        \\n        return self.rank_recommendations(recommendations)', 0), ('How do we implement rate limiting?', [\"export class RateLimiter {\\n  constructor(private redis: Redis) {}\\n\\n  async limit(req: Request, res: Response, next: NextFunction) {\\n    const key = `rate_limit:${req.ip}`;\\n    const requests = await this.redis.incr(key);\\n    \\n    if (requests === 1) {\\n      await this.redis.expire(key, 60);\\n    }\\n    \\n    if (requests > 100) {\\n      return res.status(429).json({ error: 'Too many requests' });\\n    }\\n    \\n    next();\\n  }\\n}\"], 1), ('How do we implement rate limiting?', \"export class ErrorHandler {\\n  catch(error: Error, req: Request, res: Response, next: NextFunction) {\\n    if (error instanceof ValidationError) {\\n      return res.status(400).json({\\n        type: 'ValidationError',\\n        message: error.message\\n      });\\n    }\\n    \\n    if (error instanceof AuthenticationError) {\\n      return res.status(401).json({\\n        type: 'AuthenticationError',\\n        message: 'Unauthorized'\\n      });\\n    }\\n    \\n    // Default error\\n    res.status(500).json({\\n      type: 'ServerError',\\n      message: 'Internal server error'\\n    });\\n  }\\n}\", 0), ('Where is the search functionality implemented?', [\"export class SearchService {\\n  async search(query: string, filters: SearchFilters): Promise<SearchResult[]> {\\n    const elasticQuery = this.buildElasticQuery(query, filters);\\n    \\n    const results = await this.elasticClient.search({\\n      index: 'products',\\n      body: elasticQuery\\n    });\\n    \\n    return this.mapResults(results.hits);\\n  }\\n}\"], 1), ('Where is the search functionality implemented?', 'def initiate_password_reset(email):\\n    token = generate_reset_token()\\n    send_reset_email(email, token)\\n    store_reset_token(email, token, expiry=24*hours)\\n    return True\\n\\ndef validate_reset_token(token, new_password):\\n    if is_token_valid(token):\\n        user = get_user_by_token(token)\\n        update_password(user, new_password)\\n        invalidate_token(token)\\n        return True\\n    return False', 0), ('How do we handle logging?', [\"import winston from 'winston';\\n\\nexport const logger = winston.createLogger({\\n  level: process.env.LOG_LEVEL || 'info',\\n  format: winston.format.json(),\\n  transports: [\\n    new winston.transports.File({ filename: 'error.log', level: 'error' }),\\n    new winston.transports.File({ filename: 'combined.log' })\\n  ]\\n});\"], 1), ('How do we handle logging?', 'class RecommendationEngine:\\n    def generate_recommendations(self, user_id):\\n        user_history = self.get_user_history(user_id)\\n        similar_users = self.find_similar_users(user_history)\\n        \\n        recommendations = []\\n        for user in similar_users:\\n            items = self.get_user_items(user)\\n            recommendations.extend(self.filter_unseen_items(items, user_history))\\n        \\n        return self.rank_recommendations(recommendations)', 0), ('Where is the caching logic?', [\"export class CacheService {\\n  constructor(private redis: Redis) {}\\n  \\n  async get<T>(key: string): Promise<T | null> {\\n    const cached = await this.redis.get(key);\\n    return cached ? JSON.parse(cached) : null;\\n  }\\n  \\n  async set(key: string, value: any, ttl?: number): Promise<void> {\\n    await this.redis.set(key, JSON.stringify(value), 'EX', ttl || 3600);\\n  }\\n}\"], 1), ('Where is the caching logic?', 'class RecommendationEngine:\\n    def generate_recommendations(self, user_id):\\n        user_history = self.get_user_history(user_id)\\n        similar_users = self.find_similar_users(user_history)\\n        \\n        recommendations = []\\n        for user in similar_users:\\n            items = self.get_user_items(user)\\n            recommendations.extend(self.filter_unseen_items(items, user_history))\\n        \\n        return self.rank_recommendations(recommendations)', 0), ('How do we handle websocket connections?', [\"@WebSocketGateway()\\nexport class WebSocketGateway {\\n  @WebSocketServer()\\n  server: Server;\\n\\n  handleConnection(client: Socket) {\\n    const userId = this.getUserFromToken(client.handshake.auth.token);\\n    client.join(`user:${userId}`);\\n  }\\n\\n  @SubscribeMessage('message')\\n  handleMessage(client: Socket, payload: any) {\\n    this.server.to(payload.room).emit('message', payload);\\n  }\\n}\"], 1), ('How do we handle websocket connections?', 'export class AnalyticsService {\\n  async trackEvent(event: AnalyticsEvent) {\\n    await this.segmentClient.track({\\n      userId: event.userId,\\n      event: event.name,\\n      properties: event.properties,\\n      timestamp: new Date()\\n    });\\n    \\n    await this.store.incrementEventCount(event.name);\\n  }\\n}', 0), ('Where is the input validation handled?', ['export class ValidationMiddleware {\\n  validate(schema: Joi.Schema) {\\n    return (req: Request, res: Response, next: NextFunction) => {\\n      const { error } = schema.validate(req.body);\\n      \\n      if (error) {\\n        return res.status(400).json({\\n          error: error.details[0].message\\n        });\\n      }\\n      \\n      next();\\n    };\\n  }\\n}'], 1), ('Where is the input validation handled?', \"import winston from 'winston';\\n\\nexport const logger = winston.createLogger({\\n  level: process.env.LOG_LEVEL || 'info',\\n  format: winston.format.json(),\\n  transports: [\\n    new winston.transports.File({ filename: 'error.log', level: 'error' }),\\n    new winston.transports.File({ filename: 'combined.log' })\\n  ]\\n});\", 0), ('How do we implement the shopping cart?', ['export class CartService {\\n  async addToCart(userId: string, productId: string, quantity: number) {\\n    const cart = await this.cartRepo.findByUser(userId);\\n    const product = await this.productRepo.findById(productId);\\n    \\n    if (product.stock < quantity) {\\n      throw new InsufficientStockError();\\n    }\\n    \\n    return this.cartRepo.addItem(cart.id, {\\n      productId,\\n      quantity,\\n      price: product.price\\n    });\\n  }\\n}'], 1), ('How do we implement the shopping cart?', 'export class ValidationMiddleware {\\n  validate(schema: Joi.Schema) {\\n    return (req: Request, res: Response, next: NextFunction) => {\\n      const { error } = schema.validate(req.body);\\n      \\n      if (error) {\\n        return res.status(400).json({\\n          error: error.details[0].message\\n        });\\n      }\\n      \\n      next();\\n    };\\n  }\\n}', 0), ('Where is the notification system implemented?', ['export class NotificationService {\\n  async notify(userId: string, notification: Notification) {\\n    const user = await this.userRepo.findById(userId);\\n    \\n    if (user.preferences.email) {\\n      await this.emailService.send(user.email, notification);\\n    }\\n    \\n    if (user.preferences.push) {\\n      await this.pushService.send(user.deviceToken, notification);\\n    }\\n  }\\n}'], 1), ('Where is the notification system implemented?', 'class RecommendationEngine:\\n    def generate_recommendations(self, user_id):\\n        user_history = self.get_user_history(user_id)\\n        similar_users = self.find_similar_users(user_history)\\n        \\n        recommendations = []\\n        for user in similar_users:\\n            items = self.get_user_items(user)\\n            recommendations.extend(self.filter_unseen_items(items, user_history))\\n        \\n        return self.rank_recommendations(recommendations)', 0), ('How do we handle API errors?', [\"export class ErrorHandler {\\n  catch(error: Error, req: Request, res: Response, next: NextFunction) {\\n    if (error instanceof ValidationError) {\\n      return res.status(400).json({\\n        type: 'ValidationError',\\n        message: error.message\\n      });\\n    }\\n    \\n    if (error instanceof AuthenticationError) {\\n      return res.status(401).json({\\n        type: 'AuthenticationError',\\n        message: 'Unauthorized'\\n      });\\n    }\\n    \\n    // Default error\\n    res.status(500).json({\\n      type: 'ServerError',\\n      message: 'Internal server error'\\n    });\\n  }\\n}\"], 1), ('How do we handle API errors?', 'def initiate_password_reset(email):\\n    token = generate_reset_token()\\n    send_reset_email(email, token)\\n    store_reset_token(email, token, expiry=24*hours)\\n    return True\\n\\ndef validate_reset_token(token, new_password):\\n    if is_token_valid(token):\\n        user = get_user_by_token(token)\\n        update_password(user, new_password)\\n        invalidate_token(token)\\n        return True\\n    return False', 0), ('Where is the user profile update logic?', ['export class ProfileService {\\n  async updateProfile(userId: string, updates: ProfileUpdates): Promise<User> {\\n    const user = await this.userRepo.findById(userId);\\n    \\n    if (updates.email && updates.email !== user.email) {\\n      await this.verifyEmailUnique(updates.email);\\n    }\\n    \\n    const updatedUser = await this.userRepo.update(userId, updates);\\n    return this.sanitizeUser(updatedUser);\\n  }\\n}'], 1), ('Where is the user profile update logic?', 'export class CartService {\\n  async addToCart(userId: string, productId: string, quantity: number) {\\n    const cart = await this.cartRepo.findByUser(userId);\\n    const product = await this.productRepo.findById(productId);\\n    \\n    if (product.stock < quantity) {\\n      throw new InsufficientStockError();\\n    }\\n    \\n    return this.cartRepo.addItem(cart.id, {\\n      productId,\\n      quantity,\\n      price: product.price\\n    });\\n  }\\n}', 0), ('How do we generate PDFs?', [\"export class PDFGenerator {\\n  async generatePDF(template: string, data: any): Promise<Buffer> {\\n    const html = await this.templateEngine.render(template, data);\\n    \\n    const browser = await puppeteer.launch();\\n    const page = await browser.newPage();\\n    await page.setContent(html);\\n    \\n    const pdf = await page.pdf({\\n      format: 'A4',\\n      printBackground: true\\n    });\\n    \\n    await browser.close();\\n    return pdf;\\n  }\\n}\"], 1), ('How do we generate PDFs?', \"export class SearchService {\\n  async search(query: string, filters: SearchFilters): Promise<SearchResult[]> {\\n    const elasticQuery = this.buildElasticQuery(query, filters);\\n    \\n    const results = await this.elasticClient.search({\\n      index: 'products',\\n      body: elasticQuery\\n    });\\n    \\n    return this.mapResults(results.hits);\\n  }\\n}\", 0), ('Where is the background job processing handled?', [\"export class JobProcessor {\\n  @Process('email')\\n  async processEmailJob(job: Job) {\\n    const { to, subject, content } = job.data;\\n    await this.emailService.send(to, subject, content);\\n  }\\n\\n  @Process('image-resize')\\n  async processImageResize(job: Job) {\\n    const { imageUrl, dimensions } = job.data;\\n    const resized = await this.imageService.resize(imageUrl, dimensions);\\n    await this.storageService.upload(resized);\\n  }\\n}\"], 1), ('Where is the background job processing handled?', \"export class RateLimiter {\\n  constructor(private redis: Redis) {}\\n\\n  async limit(req: Request, res: Response, next: NextFunction) {\\n    const key = `rate_limit:${req.ip}`;\\n    const requests = await this.redis.incr(key);\\n    \\n    if (requests === 1) {\\n      await this.redis.expire(key, 60);\\n    }\\n    \\n    if (requests > 100) {\\n      return res.status(429).json({ error: 'Too many requests' });\\n    }\\n    \\n    next();\\n  }\\n}\", 0), ('How do we implement the recommendation system?', ['class RecommendationEngine:\\n    def generate_recommendations(self, user_id):\\n        user_history = self.get_user_history(user_id)\\n        similar_users = self.find_similar_users(user_history)\\n        \\n        recommendations = []\\n        for user in similar_users:\\n            items = self.get_user_items(user)\\n            recommendations.extend(self.filter_unseen_items(items, user_history))\\n        \\n        return self.rank_recommendations(recommendations)'], 1), ('How do we implement the recommendation system?', \"export class CacheService {\\n  constructor(private redis: Redis) {}\\n  \\n  async get<T>(key: string): Promise<T | null> {\\n    const cached = await this.redis.get(key);\\n    return cached ? JSON.parse(cached) : null;\\n  }\\n  \\n  async set(key: string, value: any, ttl?: number): Promise<void> {\\n    await this.redis.set(key, JSON.stringify(value), 'EX', ttl || 3600);\\n  }\\n}\", 0), ('Where is the analytics tracking implemented?', ['export class AnalyticsService {\\n  async trackEvent(event: AnalyticsEvent) {\\n    await this.segmentClient.track({\\n      userId: event.userId,\\n      event: event.name,\\n      properties: event.properties,\\n      timestamp: new Date()\\n    });\\n    \\n    await this.store.incrementEventCount(event.name);\\n  }\\n}'], 1), ('Where is the analytics tracking implemented?', \"export class PDFGenerator {\\n  async generatePDF(template: string, data: any): Promise<Buffer> {\\n    const html = await this.templateEngine.render(template, data);\\n    \\n    const browser = await puppeteer.launch();\\n    const page = await browser.newPage();\\n    await page.setContent(html);\\n    \\n    const pdf = await page.pdf({\\n      format: 'A4',\\n      printBackground: true\\n    });\\n    \\n    await browser.close();\\n    return pdf;\\n  }\\n}\", 0)]\n"
     ]
    }
   ],
   "source": [
    "print(example_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training pairs: 32\n",
      "Number of validation pairs: 8\n"
     ]
    }
   ],
   "source": [
    "# Here we split the example pairs into training and validation sets\n",
    "np.random.shuffle(example_pairs)\n",
    "split_index = int(0.8 * len(example_pairs))\n",
    "train_pairs = example_pairs[:split_index]\n",
    "val_pairs = example_pairs[split_index:]\n",
    "\n",
    "print(f\"Number of training pairs: {len(train_pairs)}\")\n",
    "print(f\"Number of validation pairs: {len(val_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a loss function\n",
    "\n",
    "With a model to calculate similarity, and a dataset of positive and negative examples, we're almost ready to train. The last thing we need is a loss function. Design a loss function that is suitable for this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch.nn as nn\n",
    "# using binary cross entropy because task is binary classification\n",
    "def loss_func(predictions, targets):\n",
    "    # Error: cant call numpy() on tensor that requires grad\n",
    "    # loss = -np.mean(targets * np.log(predictions) + (1 - targets) * np.log(1 - predictions))\n",
    "    loss = nn.BCEWithLogitsLoss(predictions, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the projection matrix\n",
    "\n",
    "The entire training loop has been set up, except for a couple of lines to calculate the prediction given an example pair and to get $y$, which will then be used together to calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0: Training Loss: 0.00348901219695108\n",
      "Epoch 0/25: validation loss: 0.7879123687744141\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1: Training Loss: 0.9116001138463616\n",
      "Epoch 1/25: validation loss: 0.8445032835006714\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2: Training Loss: 0.6485633719712496\n",
      "Epoch 2/25: validation loss: 0.9067591428756714\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n",
      "Epoch 3: Training Loss: 0.4539702092297375\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#added\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Initialize the projection matrix P\n",
    "P = torch.randn(\n",
    "    dim, dim, requires_grad=True\n",
    ")\n",
    "\n",
    "# Set hyperparameters\n",
    "lr = 0.01\n",
    "num_epochs = 25\n",
    "optimizer = optim.Adam([P], lr=lr)\n",
    "epochs, types, losses, accuracies, matrices = [], [], [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = 0.0\n",
    "    # Iterate through training pairs\n",
    "    for pair in train_pairs:\n",
    "        prediction = similarity_with_projection(pair[0], pair[1], P)\n",
    "        prediction = prediction.unsqueeze(0)\n",
    "        y = torch.tensor(pair[2], dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        loss = criterion(prediction, y)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        # print(f\"Epoch {epoch}: Training Loss: {avg_loss}\")\n",
    "    \n",
    "    # Update weights using Adam optimizer\n",
    "    optimizer.step()\n",
    "    avg_loss = total_loss / len(train_pairs)\n",
    "\n",
    "    # Calculate validation loss\n",
    "    val_loss = 0\n",
    "    for pair in val_pairs:\n",
    "        prediction = similarity_with_projection(pair[0], pair[1], P)\n",
    "        prediction = prediction.unsqueeze(0)\n",
    "        y = torch.tensor(pair[2], dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        val_loss += criterion(prediction, y)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{num_epochs}: validation loss: {val_loss.item() / len(val_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval strategy\n",
    "\n",
    "We now have a potentially improved embedding model, but need to use it for retrieval. Finish the retrieval function, which will take a user input and return relevant code snippets from the full list. Note: a vector database is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"export class ErrorHandler {\\n  catch(error: Error, req: Request, res: Response, next: NextFunction) {\\n    if (error instanceof ValidationError) {\\n      return res.status(400).json({\\n        type: 'ValidationError',\\n        message: error.message\\n      });\\n    }\\n    \\n    if (error instanceof AuthenticationError) {\\n      return res.status(401).json({\\n        type: 'AuthenticationError',\\n        message: 'Unauthorized'\\n      });\\n    }\\n    \\n    // Default error\\n    res.status(500).json({\\n      type: 'ServerError',\\n      message: 'Internal server error'\\n    });\\n  }\\n}\",\n",
       "  0.47227051854133606),\n",
       " ('export class ValidationMiddleware {\\n  validate(schema: Joi.Schema) {\\n    return (req: Request, res: Response, next: NextFunction) => {\\n      const { error } = schema.validate(req.body);\\n      \\n      if (error) {\\n        return res.status(400).json({\\n          error: error.details[0].message\\n        });\\n      }\\n      \\n      next();\\n    };\\n  }\\n}',\n",
       "  0.3774653673171997),\n",
       " ('export class CartService {\\n  async addToCart(userId: string, productId: string, quantity: number) {\\n    const cart = await this.cartRepo.findByUser(userId);\\n    const product = await this.productRepo.findById(productId);\\n    \\n    if (product.stock < quantity) {\\n      throw new InsufficientStockError();\\n    }\\n    \\n    return this.cartRepo.addItem(cart.id, {\\n      productId,\\n      quantity,\\n      price: product.price\\n    });\\n  }\\n}',\n",
       "  0.21208922564983368),\n",
       " (\"export class RateLimiter {\\n  constructor(private redis: Redis) {}\\n\\n  async limit(req: Request, res: Response, next: NextFunction) {\\n    const key = `rate_limit:${req.ip}`;\\n    const requests = await this.redis.incr(key);\\n    \\n    if (requests === 1) {\\n      await this.redis.expire(key, 60);\\n    }\\n    \\n    if (requests > 100) {\\n      return res.status(429).json({ error: 'Too many requests' });\\n    }\\n    \\n    next();\\n  }\\n}\",\n",
       "  0.17327788472175598),\n",
       " (\"import winston from 'winston';\\n\\nexport const logger = winston.createLogger({\\n  level: process.env.LOG_LEVEL || 'info',\\n  format: winston.format.json(),\\n  transports: [\\n    new winston.transports.File({ filename: 'error.log', level: 'error' }),\\n    new winston.transports.File({ filename: 'combined.log' })\\n  ]\\n});\",\n",
       "  0.1609550416469574)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_snippets = []\n",
    "\n",
    "for example in dataset:\n",
    "    for snippet in example.snippets:\n",
    "        all_snippets.append(snippet)\n",
    "\n",
    "# Use similarity search with the embeddings model to retrieve relevant snippets\n",
    "def retrieve_relevant_snippets(user_input: str):\n",
    "    ui_encode = model.encode(user_input)\n",
    "    ui_embed = torch.tensor(ui_encode, dtype=torch.float32)\n",
    "\n",
    "    snip_encode = model.encode(all_snippets)\n",
    "    snip_embed = torch.tensor(snip_encode, dtype=torch.float32)\n",
    "\n",
    "    cos_sim = nn.functional.cosine_similarity(ui_embed, snip_embed)\n",
    "\n",
    "    top_k = torch.topk(cos_sim, k=5).indices\n",
    "\n",
    "    relevant_snippets = [(all_snippets[idx], cos_sim[idx].item()) for idx in top_k]\n",
    "\n",
    "    return relevant_snippets\n",
    "\n",
    "    \n",
    "retrieve_relevant_snippets(\"How do we handle API errors?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the new retrieval strategy\n",
    "\n",
    "If the loss was lower by the last epoch, then we know that we improved the similarity function (at least for the validation set), but we still need a way of evaluating the retrieval strategy as a whole.\n",
    "\n",
    "Your last task is to design an evaluation metric suitable for codebase retrieval, which we can run over the examples in the above dataset. The result of the evaluation should be a single number that attempts to represent the quality of the retrieval strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def evaluate_retrieval_strategy(retrieval_strategy):\n",
    "    total_rank = 0\n",
    "    for example in dataset:\n",
    "        user_input = example.user_input\n",
    "        current_snippet = example.snippets\n",
    "        relevant_snippets = retrieval_strategy(user_input)\n",
    "\n",
    "        reciprocal_rank = 0\n",
    "        for rank, (snippet, _) in enumerate(relevant_snippets):\n",
    "            if snippet in current_snippet:\n",
    "                reciprocal_rank = 1 / (rank + 1)\n",
    "                break\n",
    "        total_rank += reciprocal_rank\n",
    "\n",
    "\n",
    "    mrr = total_rank / len(dataset)\n",
    "    return mrr\n",
    "\n",
    "result = evaluate_retrieval_strategy(retrieve_relevant_snippets)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
