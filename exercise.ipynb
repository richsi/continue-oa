{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing codebase embeddings with a projection matrix\n",
    "\n",
    "## Goal\n",
    "\n",
    "The outcome of this exercise is to learn a projection matrix that tailors embeddings for a codebase retrieval use case, and then measure the improvement in retrieval quality.\n",
    "\n",
    "The notebook is mostly filled out, but has a series of small gaps that you will need to fill in (everywhere you see a \"TODO\" comment):\n",
    "- Define the similarity functions (both basic and with projection matrix)\n",
    "- Define a suitable loss function\n",
    "- Construct examples for training from the pre-existing dataset\n",
    "- Complete the training loop code\n",
    "- Finish the retrieval function logic\n",
    "- Evaluate the improvement in retrieval quality\n",
    "\n",
    "## Background\n",
    "\n",
    "A basic retrieval augmented generation (RAG) system will typically use embeddings to represent a set of documents that are to be searched over. Then the user input can also be converted to an embedding, and the system will use the dot product of the two embeddings to determine the relevance of the input to the documents in the database.\n",
    "\n",
    "Many embedding models are \"symmetric\", which means that they treat user input text and documents (e.g. code snippets) in the same way. It might be preferable to calculate the embedding differently (\"asymmetrically\") for the user input because is is a fundamentally different type of text.\n",
    "\n",
    "One way of doing this is to use the same embedding model, and then apply a matrix multiplication to the embedding of the user input. What we'll try to do here is find such a matrix that can improve retrieval quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "We recommend using a virtual environment to install the necessary packages.\n",
    "\n",
    "```bash\n",
    "python -m venv env\n",
    "source env/bin/activate\n",
    "```\n",
    "\n",
    "### Install packages\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Here we generate a sample embedding with `sentence_transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained model (this will be slow the first time)\n",
    "model = SentenceTransformer(\"all-MiniLM-L12-v2\")\n",
    "\n",
    "def embed(text):\n",
    "    embedding = model.encode([text])[0]\n",
    "    return torch.tensor(embedding, dtype=torch.float32)\n",
    "\n",
    "embedding = embed(\"Hello world\")\n",
    "dim = len(embedding)\n",
    "\n",
    "print(f\"Embedding dimension: {dim}\")\n",
    "print(f\"Embedding: [{embedding[0]}, {embedding[1]}, ..., {embedding[-1]}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity\n",
    "\n",
    "First, we'll define our definition of similarity. This can be calculated using a dot product between two embeddings. For example, if we were trying to find the similarity between a user input $x_i$ and a code snippet $x_c$, then the similarity would be\n",
    "\n",
    "$$h(x_i, x_c) = e(x_i) \\cdot e(x_c)$$\n",
    "\n",
    "Fill out the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the similarity function using torch\n",
    "def similarity(x_i, x_c):\n",
    "    # TODO\n",
    "    raise NotImplementedError\n",
    "\n",
    "# Calculate the similarity between two strings\n",
    "x_i = \"Where in the codebase do we do auth?\"\n",
    "x_c_1 = \"```python\\n# Authentication\\ndef authenticate(username, password):\\n    # Code to authenticate the user\\n```\"\n",
    "x_c_2 = \"function sum(a, b) {\\n    return a + b;\\n}\"\n",
    "\n",
    "similarity1 = similarity(x_i, x_c_1)\n",
    "similarity2 = similarity(x_i, x_c_2)\n",
    "print(f\"Similarity 1: {similarity1}\")\n",
    "print(f\"Similarity 2: {similarity2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity with projection matrix\n",
    "\n",
    "Next, we'll calculate similarity using the projection matrix\n",
    "\n",
    "$$h_\\theta(x_i, x_c) = e(x_c) \\theta e(x_i)$$\n",
    "\n",
    "Fill in the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_with_projection(x_i, x_c, P):\n",
    "    # TODO\n",
    "    raise NotImplementedError\n",
    "\n",
    "# Generate a dim by dim random matrix\n",
    "P_random = torch.randn(dim, dim, dtype=torch.float32)\n",
    "print(P_random)\n",
    "\n",
    "# Calculate the similarity with the random projection matrix\n",
    "similarity_with_projection1 = similarity_with_projection(x_i, x_c_1, P_random)\n",
    "similarity_with_projection2 = similarity_with_projection(x_i, x_c_2, P_random)\n",
    "print(f\"Similarity with projection 1: {similarity_with_projection1}\")\n",
    "print(f\"Similarity with projection 2: {similarity_with_projection2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "To train and test a matrix that is more helpful than the random one above, we will use a pre-existing dataset, which includes a list of (question, relevant code snippets) pairs, which happen to have been generated by a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from XML file (dataset.xml)\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass \n",
    "class Example:\n",
    "    user_input: str\n",
    "    snippets: List[str]\n",
    "\n",
    "class DatasetParser:\n",
    "    def __init__(self, xml_file: str):\n",
    "        self.tree = ET.parse(xml_file)\n",
    "        self.root = self.tree.getroot()\n",
    "\n",
    "    def parse(self) -> List[Example]:\n",
    "        examples = []\n",
    "        \n",
    "        for example in self.root.findall('example'):\n",
    "            user_input = example.find('user_input').text\n",
    "            snippets_list = []\n",
    "            \n",
    "            for snippet in example.find('snippets').findall('snippet'):\n",
    "                # Extract code and filename from the snippet text\n",
    "                snippet_text = snippet.text.strip()\n",
    "                \n",
    "                # Parse the filename from the code block header\n",
    "                first_line = snippet_text.split('\\n')[0]\n",
    "                filename = first_line.split(' ')[1] if len(first_line.split(' ')) > 1 else None\n",
    "                \n",
    "                # Remove the code block markers and get just the code\n",
    "                code_lines = snippet_text.split('\\n')[1:-1]\n",
    "                code = '\\n'.join(code_lines)\n",
    "                \n",
    "                snippets_list.append(code)\n",
    "                \n",
    "            examples.append(Example(\n",
    "                user_input=user_input,\n",
    "                snippets=snippets_list\n",
    "            ))\n",
    "            \n",
    "        return examples\n",
    "\n",
    "\n",
    "parser = DatasetParser('dataset.xml')\n",
    "dataset = parser.parse()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct examples\n",
    "\n",
    "Convert the dataset into a set of examples that can be used to train the projection matrix. These should include both examples of input/snippet pairs where the snippet is relevant, and pairs where the snippet is not relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, you should generate a list of positive and negative pairs from the dataset\n",
    "# These will be used to train the matrix\n",
    "\n",
    "# TODO: Create example pairs from the dataset\n",
    "example_pairs = []  # list of tuples (user input, code snippet, 1 if snippet is relevant to user input else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we split the example pairs into training and validation sets\n",
    "np.random.shuffle(example_pairs)\n",
    "split_index = int(0.8 * len(example_pairs))\n",
    "train_pairs = example_pairs[:split_index]\n",
    "val_pairs = example_pairs[split_index:]\n",
    "\n",
    "print(f\"Number of training pairs: {len(train_pairs)}\")\n",
    "print(f\"Number of validation pairs: {len(val_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a loss function\n",
    "\n",
    "With a model to calculate similarity, and a dataset of positive and negative examples, we're almost ready to train. The last thing we need is a loss function. Design a loss function that is suitable for this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(predictions, targets):\n",
    "    # TODO\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the projection matrix\n",
    "\n",
    "The entire training loop has been set up, except for a couple of lines to calculate the prediction given an example pair and to get $y$, which will then be used together to calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize the projection matrix P\n",
    "P = torch.randn(\n",
    "    dim, dim, requires_grad=True\n",
    ")\n",
    "\n",
    "# Set hyperparameters\n",
    "lr = 0.1\n",
    "num_epochs = 25\n",
    "optimizer = optim.Adam([P], lr=lr)\n",
    "epochs, types, losses, accuracies, matrices = [], [], [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Iterate through training pairs\n",
    "    for pair in train_pairs:\n",
    "        # TODO: Get `prediction` and `y` to pass to `loss_func`\n",
    "        prediction = 0 # <-- TODO\n",
    "        y = 0 # <-- TODO\n",
    "\n",
    "        loss = loss_func(prediction, y)\n",
    "        loss.backward()\n",
    "    \n",
    "    # Update weights using Adam optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculate validation loss\n",
    "    val_loss = 0\n",
    "    for pair in val_pairs:\n",
    "        # TODO: Get `prediction` and `y` to pass to `loss_func`\n",
    "        prediction = 0 # <-- TODO\n",
    "        y = 0 # <-- TODO\n",
    "        \n",
    "        val_loss += loss_func(prediction, y)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{num_epochs}: validation loss: {val_loss.item() / len(val_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval strategy\n",
    "\n",
    "We now have a potentially improved embedding model, but need to use it for retrieval. Finish the retrieval function, which will take a user input and return relevant code snippets from the full list. Note: a vector database is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_snippets = []\n",
    "\n",
    "for example in dataset:\n",
    "    for snippet in example['snippets']:\n",
    "        all_snippets.append(snippet)\n",
    "\n",
    "# Use similarity search with the embeddings model to retrieve relevant snippets\n",
    "def retrieve_relevant_snippets(user_input: str):\n",
    "    # TODO\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the new retrieval strategy\n",
    "\n",
    "If the loss was lower by the last epoch, then we know that we improved the similarity function (at least for the validation set), but we still need a way of evaluating the retrieval strategy as a whole.\n",
    "\n",
    "Your last task is to design an evaluation metric suitable for codebase retrieval, which we can run over the examples in the above dataset. The result of the evaluation should be a single number that attempts to represent the quality of the retrieval strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval_strategy(retrieval_strategy):\n",
    "    # TODO\n",
    "    raise NotImplementedError\n",
    "\n",
    "result = evaluate_retrieval_strategy(retrieve_relevant_snippets)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
